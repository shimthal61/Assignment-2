---
title: "Assignment 2"
author: "Student ID: 10179889"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    font-family: Lato
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

For this assignment, we have been provided with three different datasets. Using the knowledge we have gained from the workshops combined with wider reading, I will:

-   Wrangle and tidy the data
-   Summarise the data
-   Visualise the data
-   Build and interpret appropriate analyses

## Packages

First, let's load in our packages using the `library()` function:

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(showtext)
library(afex)
library(emmeans)
library(visdat)
library(ggthemes)
```

The `{tidyverse}` package contains a collection of open source R packages that gives us access to a variety of useful functions and commands for data wrangling and visualisation. The `{showtext}` package enables us to load in, and use over 1291 fonts for our data visualisations from [Google Fonts](https://fonts.google.com/), whilst `{ggtheme}` contains extra themes for our plots. The `{afex}`and`{emmeans}` packages will allow us to build ANOVA models and perform post-hoc analyses, respectively. We'll be using the `{afex]` package instead of the `{aov]` function in base R as it allows us to build ANOVA models that will work for our experimental designs, as well as using type III sums of squares by default. Finally, `{visdat}` will help us to spot any missing data in our datasets.

# Question 1

Let's read in our data and look over the question 1 information:

> <font size="3"> We have 96 participants in a between participants design where we are interested in the effect of visual quality of a word on response time to pronounce the word. Our experimental factor (visual quality) has 2 levels (Normal vs. Degraded). The time it took for participants to pronounce the word, which was taken as a measure of reaction time in milliseconds, served as the dependent variable. Let's read in our data: </font>

```{r, message = FALSE}
raw_data_1 <- read_csv("assignment_2_dataset_1.csv")
head(raw_data_1)
```

## Data Wrangling

Our Visual Quality variable is currently just named "condition" - let's change it using the `rename()` function. Within the `mutate()` function, we can also modify it so it's coded as a factor. Finally, our conditions are not labelled meaningfully and could lead to confusion further down the line. We can recode them using the `recode` command:

```{r, message = FALSE}
q1_data_tidied <- raw_data_1 %>%
  rename(visual_quality = condition) %>% 
  mutate(visual_quality = recode(visual_quality,
                            "condition_a" = "Normal",
                            "condition_b" = "Degraded")) %>% 
  mutate(visual_quality = factor(visual_quality))
head(q1_data_tidied)
```


## Data Summarising

We'll first utilise the `group_by()` function to gather our visual-quality variable, then `summarise()` to create a new table consisting of Response Time mean and standard deviation. The data can be arranged from the fastest mean Response Time to the slowest using the `arrange()` function:

```{r, message = FALSE}
q1_data_tidied %>% 
  group_by(visual_quality) %>% 
  summarise(mean = mean(response_time), sd = sd(response_time)) %>% 
  arrange(mean)
```

At a glance, it looks as though participants who were presented a word with normal visual quality were faster at pronouncing the word than those who were presented with a word with degraded quality. Since the output returned no NA results, we know that there is no missing data and therefore no reason to use the `vis_dat() function`. At this stage, we can create some data visualisations to better present our results.

## Data Visualisations

In this visualisation, we're jittering the data points, meaning that every time we execute the code, the points will jitter to a different position. This implies that our code is not reproducible, because we don't know the seed that R will use to generate the sequence. By setting the seed using the `set.seed()` function, we can ensure that the output will be the same every time it is run. I set the seed to the number 42 for its reference to Hitch Hikers Guide to the Galaxy...

```{r}
set.seed(42)
```

To be able to manipulate the font family, we first need to download a font from [Google Fonts](https://fonts.google.com). I decided to pick the 'Lato' Font, as its larger character height lends itself to easier readability at small sizes. Let's read this font into R using the `font_add_()` command and tell `{showtext}` to automatically render the text:

```{r}
font_add("lato", regular = "lato-regular.ttf")
showtext_auto()
```

Next, we can build a visualisation by plotting the raw data points using the `geom_point` function, and the shape of the distribution for each condition using the `geom_violin()` function. By default, R will order the conditions alphabetically, so I used the `fct_relevel()` function to reorder the labels on the x-axis so that our 'normal' condition (the baseline) is on the left hand side. We have also added some summary data in the form of the Mean and Confidence Intervals around the mean using the `stat_summary()` function. Finally, we can dictate the y-axis breaks within the `scale_y_continuous()` function, and change the text font and size within the `theme()` argument:

```{r, message = FALSE, warning = FALSE}
q1_data_tidied %>% 
  mutate(visual_quality = fct_relevel(visual_quality, "Normal", "Degraded")) %>% 
  ggplot(aes(x = visual_quality, y = response_time, colour = visual_quality)) +
  geom_violin(width = 0.3) +
  geom_point(alpha = 0.7, position = position_jitter(width = 0.08, seed = 42)) +
  theme_hc() +
  stat_summary(fun.data = "mean_cl_boot", colour = "black") +
  guides(colour = 'none') +
  scale_y_continuous(breaks = seq(925, 1075, by = 25),
                     limits = c(925, 1075)) +
  labs(title = "Examining the effect of Visual Quality on Response Times",
       x = "Visual Quality",
       y = "Response Time (ms)") +
  theme(text = element_text(family = "Lato", size = 25))
```

## Building our ANOVA Model

Let's now build our model using the `aov_4()` function in the `{afex}` package. Within this package, the syntax for ANOVA models is: `aov_4(DV ~ IV + (1 | participant), data = q1_data_tidied)`. The `~` symbol translates to 'predicted by', and takes into account our random effect. We need to specify what data we are using in our model using `data = q1_data_tidied`. Finally, we map the output of the ANOVA result onto a variable I've called `between_anova`. This means that the ANOVA results will be stored in this variable and will allow us to access them later.

```{r, message=FALSE, warning = FALSE}
between_anova <- aov_4(response_time ~ visual_quality + (1 | participant), data = q1_data_tidied)
```

## Interpreting the Model Output

We can view the output of our new ANOVA model using the `anova()` function:

```{r}
anova(between_anova)
```

We can see that there is an effect in our model - the *p*-value is pretty small, but we don't yet know what direction this is in. Although we could refer to the condition means, I'm going to run a pairwise comparison using the `emmeans()` function. There is no need to change the adjustment, as we're not running multiple comparisons.

```{r}
emmeans(between_anova, pairwise ~ visual_quality)
```

An independent one-way ANOVA revealed a significant difference between IV level means, *F*(1,94) = 15.78, *p* \< .001, generalised η2 = .14. Tukey comparisons revealed that the normal visual quality group performed significantly better than the degraded visual quality group (*p* \< .001)

In other words, participants were faster at pronouncing a word when the word was visually normal, compared to when the word was visually degraded.

# Question 2

First, we should read in our data and look over some of the characteristics of question 2:

> <font size="3"> We have 96 participants in a between participants design where we are interested in the effect of visual quality of a word on response time to pronounce the word. We are also interested in whether caffeine consumption will contribute to any of the variance. Our experimental factor (Visual Quality) has 2 levels. These are Normal vs. Degraded, and Response Time is our DV measured on a continuous scale. Caffeine Consumption is our covariate and has three levels - 1 cup, 2 cups, and 3 cups. We'll next read in our data: </font>

```{r, message=FALSE}
raw_data_2 <- read_csv("assignment_2_dataset_2.csv")
head(raw_data_2)
```

## Data Wrangling

Similar to before, we need to rename our 'condition' factor to 'visual_quality', code it as a factor, and name the conditions meaningfully:

```{r, message=FALSE}
q2_data_tidied <- raw_data_2 %>% 
  rename(visual_quality = condition) %>% 
  mutate(visual_quality = recode(visual_quality,
                                 "condition_a" = "Normal",
                                 "condition_b" = "Degraded")) %>%
  mutate(visual_quality = factor(visual_quality))
head(q2_data_tidied)
```

## Data Visualising

Let's now have a look at the relationship between our experimental factor (Visual Quality), covariate (Caffeine Consumption), and our dependent variable (Response Time).

```{r, message = FALSE, warning=FALSE}
q2_data_tidied %>%
  mutate(visual_quality = fct_relevel(visual_quality, "Normal", "Degraded")) %>% 
  ggplot(aes(x = caffeine, y = response_time, colour = visual_quality)) +
  geom_smooth(aes(x = caffeine, y = response_time), inherit.aes = FALSE,
              method = "lm", se = FALSE) +
  geom_point(size = 1.5, position = position_jitter(width = 0.08, seed = 42)) +
  theme_minimal() +
  labs(x = "Cups of coffee",
       y = "Response Time (ms)",
       colour = "Visual Quality") +
  scale_y_continuous(breaks = seq(950, 1075, by = 25),
                     limits = c(950, 1075)) +
    scale_x_discrete(breaks = seq(0, 6, by = 1),
                   limits = seq(0, 6)) +
  theme(text = element_text(family = "lato", size = 25))
```

In this visualisation, I have added in a regression line that depicts the correlation between our covariant, Caffeine, and our DV, Response Time, using the `geom_smooth()` function. In our `geom_smooth` command, I was able to remove our experimental factor, Visual Quality, from the regression line by specifying what data to include using the `aes()` function, and for this data to supersede our graph aesthetics using `inherit.aes = FALSE`.

We can see from our regression line that there isn't a particularly strong relationship between Caffeine and Response Time. Interestingly, our Caffeine consumption groups appear to be clustering in our data by Visual Quality, suggesting that our covariate may have an effect on the relationship between Visual Quality and Caffeine. Let's build an ANCOVA model to test this assumption.

## Building our ANCOVA Model

We saw in our first dataset that Visual Quality had a significant effect on Response Time, with participants who were presented with visually normal words pronouncing them quicker than participants who were presented with visually degraded words. However, let's control for the effect of our covariate by building an ANCOVA model which includes Caffeine, ensuring that we add our covariate before our experimental condition manipulation. We set the factorize parameter to FALSE so that Caffeine is treated as a continuous predictor, rather than an experimental factor in our model.

```{r, message=FALSE, warning=FALSE}
model_ancova <- aov_4(response_time ~ caffeine + visual_quality + (1 | participant), data = q2_data_tidied, factorize = FALSE)
anova(model_ancova)
```

Now, when we control the effect of our covariate (Caffeine), we can see that the effect of Visual Quality, that previously was *F* = 15.78, is now *F* = 3.57, and our *p* value is no longer significant (*p* = .062). Moreover, as we assumed in our visualisation, our covariate is also non-significant, *F* = 1.12, *p* = .293. Following on from this, we can use the `emmeans()` function to produce the adjusted means for each of our two experimental groups, taking into account the influence of our covariate. I will also perform an unadjusted marginal means test without the covariate as I did in question 1 so that we can compare them.

**Unadjusted means**

```{r}
emmeans(between_anova, pairwise ~ visual_quality)
```

**Adjusted means**

```{r}
emmeans(model_ancova, pairwise ~ visual_quality)
```

We can see that our adjusted means differ from our unadjusted means for our experimental groups, although this difference is fairly small. Furthermore, our two adjusted means are still fairly far apart (normal = 1005, degraded = 1018), and can probably account for why our *p* value (*p* = .062) was very close to the significance threshold of .05.

## ANO(C)OVA as a Special Case of Regression

We are now going to look at AN(C)OVA as a special case of regression, building the equivalent linear models and interpreting the outputs.

### Setting up our contrasts

We first need to use dummy (treatment) data for the levels of our experimental factor. Let's have a look at our contrasts using the `contrasts()` function.

```{r}
contrasts(q2_data_tidied$visual_quality)
```

As it currently stands, our 'Degraded' condition is set at our reference level (0). However, it makes more sense that our 'Normal' group be the reference level, as we want it to correspond to the intercept of our linear model. By default, R will generate a contrast matrix in alphabetical order, so we need to tell it otherwise. Similar to what we did in our visualisations, we can code the 'Normal' group as '0' using the `fct_relevel()` function:

```{r, warning=FALSE}
q2_data_tidied <- q2_data_tidied %>% 
  mutate(visual_quality = fct_relevel(visual_quality,
                                      c("Normal", "Degraded")))
contrasts(q2_data_tidied$visual_quality)
```

This is better - our 'Normal' condition is coded as '0' and now represents our intercept.

### ANOVA as a Linear Model

To make sense of our ANOVA as a Linear Model, we can use the equation of the general linear model.

`Response Time = Intercept + β1(Degraded)`

The intercept is our reference category (Normal) with coding (0), while the coding for 'Degraded' is (1). When we build our linear model, we'll be able to calculate the coefficients using this equation.

```{r}
anova_lm <- lm(response_time ~ visual_quality, data = q2_data_tidied)
```

Using the `lm()` function to build our linear model, we are asking for our DV, Response Time, to be predicted by our experimental factor, Visual Quality. Let's now call for the output of our model, which I've mapped to a variable called `anova_lm`:

```{r}
anova_lm
```

The intercept is 1002.22 (which is the mean for our 'Normal' group), and β1 is 18.09. We can now use this coefficient alongside our equation to calculate the averages for our 'Degraded' condition. I am using the `round()` function to round our intercept and coefficient means to the nearest integer, as this is what our `emmeans()` function does.

```{r}
round(1002.22 + (18.09*1))
```

This output is the same as the mean Response Time that we calculated earlier for our 'Degraded' condition. By building our linear model and using the appropriate coding scheme, we have been able to generate exactly the same means as our ANOVA. This illustrates how ANOVA is a special case of regression and uses the linear model.

### ANCOVA as a Linear Model

We can also look at our ANCOVA as a case of regression. To do this, we simply add the covariate (Caffeine Consumption) to our model specification, making sure that it precedes our experimental factor.

```{r}
ancova_lm <- lm(response_time ~ caffeine + visual_quality, data = q2_data_tidied)
ancova_lm
```

In our output, we have our intercept and our coefficient means. We can check these adjusted means against our previous ANCOVA model. Since Caffeine Consumption is not a factor we need to find out the mean of this variable using the `mean()` function:

```{r}
mean(q2_data_tidied$caffeine)
```

We add this mean (2.552083) to our equation together with the coefficients for each of our predictors. With our dummy coding scheme, we can work out the adjusted mean for our 'Normal' and 'Degraded' group. Again, I'm using the `round()` function to round the output to the nearest integer.

`Response Time = Intercept + β1(Caffeine) + β2(Degraded)`

**Normal Adjusted Mean**

```{r}
round(1011.354 + (2.469*2.552083) + (-12.791*0))
```

**Degraded Adjusted Mean**

```{r}
round(1011.354 + (2.469*2.552083) + (-12.791*1))
```

As we predicted, these adjusted means are the same as the means we saw earlier when building our ANCOVA model, and again demonstrates how ANCOVA is a special case of regression.

## Centering our Covariate

We can improve the interpretation of the coefficients in our linear model by scaling and centering our covariates. With our mean centered on 0, this standardises the variable and eliminates the need to multiply the linear model coefficient for the covariant by the covariate's mean.

By using the `scale()` function, we can create a scaled and centered version of our covariate in our data frame.

```{r}
q2_scaled_data <- q2_data_tidied %>% 
  mutate(centred_caffeine = scale(caffeine))
```

We can compare the uncentered to the centered covariate using the `plot()` function to see that the data is unchanged, other than the variable mean now being centered on zero and the distribution being scaled.

**Uncentered covariate**

```{r}
plot(density(q2_scaled_data$caffeine))
```

**Centered covariate**

```{r}
plot(density(q2_scaled_data$centred_caffeine))
```

Finally, let's build our linear model with the scaled and centered covariate.

```{r}
q2_ancova_centred <- lm(response_time ~ centred_caffeine + visual_quality, data = q2_scaled_data)
q2_ancova_centred      
```

The intercept now corresponds to the adjusted mean for the 'Normal' group. We can calculate the adjusted mean for the 'Degraded' group by subtracting 12.791 from 1017.655, and rounding the output. Therefore, scaling and centering the covariate makes it much easier to later interpret the coefficients of our linear model.

# Question 3

For our final question, let's have a look at the experiment we've been given and read in our data:

> <font size="3"> We have 148 participants in a 2 x 2 repeated measures design. We are interested in whether people respond differently to a target image following a prime image when we manipulate the image to be either positive or negative in valence. Two independent variables were manipulated within-participants in a fully factorial design; prime image with two levels (positive and negative), and target image with two levels (positive and negative). The time it took for participants to respond to the image, which was taken as a measure of reaction time in milliseconds, served as the dependent variable. </font>

```{r, message=FALSE}
raw_data_3 <- read_csv("assignment_2_dataset_3.csv")
head(raw_data_3)
```

## Data Wrangling

Our data is currently in wider format, meaning that each row represents one participant. Instead, we want each row to represent one observation from each participant. To do this, we can lengthen the data using the `pivot_longer()` function.

```{r}
longer_data <- raw_data_3 %>% 
  pivot_longer(cols = c(positiveprime_positivetarget, positiveprime_negativetarget,
                        negativeprime_positivetarget, negativeprime_negativetarget),
               names_to = "Condition",
               values_to = "reaction_time")
head(longer_data)
```

Our variables and conditions are still not labelled meaningfully - we can rename them using the `recode()` function. We also need to separate our 'Condition' variable and name them according to our independent variables so our tibble reflects the 2 x 2 structure of the experiment. The `separate()` function allows us to split 'Prime' and 'Target' into two different factors. Finally, we'll recode our IVs as factors:

```{r}
q3_data_tidied <- longer_data %>% 
  mutate(Condition = recode(Condition,
                            "positiveprime_positivetarget" = "Positive_Positive",
                            "positiveprime_negativetarget" = "Positive_Negative",
                            "negativeprime_positivetarget" = "Negative_Positive",
                            "negativeprime_negativetarget" = "Negative_Negative")) %>% 
  separate(col = "Condition", into = c("prime_valence", "target_valence"), sep = "_") %>% 
  mutate(prime_valence = factor(prime_valence), target_valence = factor(target_valence))
head(q3_data_tidied)
```

## Data Summarising

Let's generate some summary statistics and arrange the data from the fastest mean reaction time to the slowest. We need to specify our two grouping variables in the `group()` function call. I've mapped the output to a variable I've called `descriptive_stats` for later use in my visualisation.

```{r, message=FALSE}
descriptive_stats <- q3_data_tidied %>% 
  group_by(prime_valence, target_valence) %>% 
  summarise(mean_RT = mean(reaction_time), sd_RT = sd(reaction_time)) %>% 
  arrange(mean_RT)
head(descriptive_stats)
```

Looking at the tibble, it seems like participants were quickest at identifying the image when the prime valence was congruent with the target valence. As before, our output has not generated any missing data, so no need to run the `vis_miss()` function. Let's generate some data visualisations next.

## Data Visualisations

```{r}
q3_data_tidied %>% 
  ggplot(aes(x = prime_valence:target_valence, y = reaction_time, colour = target_valence)) +
  geom_violin(width = .5) +
  geom_point(alpha = 0.5, position = position_jitter(width = 0.08, seed = 42)) +
  theme_minimal() +
  labs(x = "Prime Valence", 
       y = "Reaction Time (ms)",
       colour = "Target Valence") +
  stat_summary(fun.data = "mean_cl_boot", colour = "black") +
  theme(text = element_text(size = 13)) +
  scale_y_continuous(breaks = seq(1400, 1750, by = 50),
                     limits = c(1400, 1750)) +
  scale_x_discrete(labels = c("Negative:Negative" = "Negative",
                              "Negative:Positive" = "Negative",
                              "Positive:Negative" = "Positive",
                              "Positive:Positive" = "Positive")) +
  theme(text = element_text(family = "lato", size = 25))
```

In this visualisation, I have plotted our 'Prime' condition along the x-axis and labelled them accordingly using the `scale_x_discrete()` function. I have also used the legend and an appropriate colour scheme to indicate which data represent our 'Target' condition. However, it is difficult to see any interactions in this visualisation - we'll need an interaction plot for this.

First, we'll bring the legend closer to the lines by writing the labels of the lines directly behind the lines. To do this, we'll create a new dataset that includes the y-values of the points where both lines end. We also need the labels that will be at the end of the lines. For this we use the function `case_when()`.

```{r}
labels <- descriptive_stats %>%
  filter(prime_valence == "Positive") %>% 
  mutate(label = case_when(target_valence == "Negative" ~ "Negative Target Valence",
                           target_valence == "Positive" ~ "Positive Target Valence"))
head(labels)
```

Using our `descriptive_stats` dataset, we can add the geom_text to our interaction plot and pass our dataset `labels` to the geom. Among the aesthetics, `geom_text()` includes the aesthetic label, which stands for the text we will add to the visualization. We've also scaled down the y-axis using the `scale_y_continuous()` function to visually increase the distance between the means, aiding interpretation.

```{r}
descriptive_stats %>%
  ggplot(aes(x = prime_valence, y = mean_RT)) + 
  geom_line(size = 1.2, aes(group = target_valence, colour = target_valence)) +
  geom_point(size = 2.6, aes(colour = target_valence), shape = 15) +
  geom_text(size = 8, aes(label = label,
                          colour = target_valence),
            data = labels,
            nudge_x = 0.28,
            nudge_y = 1.3) +
  guides(colour = 'none') +
  scale_y_continuous(breaks = seq(1545, 1570, by = 5),
                     limits = c(1545, 1570)) +
  labs(x = "Prime Valence",
       y = "Reaction Time (ms)") +
  theme_hc() +
  theme(text = element_text(family = "lato", size = 25))
```

We can see here that we have a crossover interaction as the polarity of the difference flips. The graph shows that Reaction Time was faster for positive prime images when the target was also positive. Likewise, Reaction Time was faster for negative prime images when the target was also negative. Due to our crossover interaction, it is unlikely that we'll have significant main effects of both Prime and Target factors, but a significant interaction effect is likely. Let's build an ANOVA model to test these predictions.

## Building our ANOVA model

The syntax is very similar to what we ran previously, although this time we need to specify our two experimental factors using the term Prime \* Target. This term corresponds to a main effect of Prime image, and a main effect of target image, plus the interaction between the two factors. Our factors are both repeated measures factors, so we also have to specify them in our random effect term in the same way.

```{r}
factorial_anova <- aov_4(reaction_time ~ prime_valence * target_valence + (1 + target_valence * prime_valence | participant), data = q3_data_tidied)
anova(factorial_anova)
```

Just as we predicted from interpreting our interaction plot, we don't have a significant main effect of Prime or a significant main effect of Target. However, we do have a significant interaction between our two experimental variables. Let's conduct some pairwise comparisons to find out where the differences lie. We don't need to change the error correction judgement as only some of the comparisons have any theoretical meaning.

```{r}
emmeans(factorial_anova, pairwise ~ prime_valence * target_valence, adjust = "none")
```

In this output, we are interested in comparing Reaction Times to Negative Targets preceded by Negative vs. Positive Primes. We are also interested in comparing Reaction Times to Positive Targets preceded by Negative vs. Positive Primes. Therefore, the two key comparisons are:

-   `Negative Negative - Positive Negative`
-   `Negative Positive - Positive Positive`

Now that we know what our meaningful comparisons are, we can manually perform a Bonferroni correction test to correct for multiple comparisons. To do this, we multiple the corresponding *p*-values by 2 (which is the number of meaningful comparisons we have). We also need to put a maximum limit of 1 on our *p*-value, as *p* can never exceed 1. 

```{r}
sum(0.0021*2)
```

```{r}
sum(0.0042*2)
```

For our first meaningful comparison, which is Reaction Time to Negative Targets when preceded by Negative vs. Positive Primes, we get a corrected *p*-value of .004. For our second meaningful comparison, which is Reaction Time to Positive Targets when preceded by Negative vs. Positive Primes, we get a corrected *p*-value of .008.

We conducted a 2 (Prime: Positive vs. Negative) x 2 (Target: Positive vs. Negative) repeated measures ANOVA to investigate the influence of Context valence on reaction times to images of Positive or Negative valence. The ANOVA revealed no effect of Prime (*F* < 1), no effect of Target (*F* < 1), but an interaction between Prime and Target (*F*(1, 147) = 17.25, *p* < .001, ηG2 = .029).

The interaction was interpreted by conducting Bonferroni-corrected pairwise comparisons. These comparisons revealed that the interaction was driven by Negative Targets being processed faster when preceded by Negative vs. Positive Primes (1,547 ms. vs. 1,567 ms., *t*(294) = 3.14, *p* = .004). The interaction was also driven by Positive Targets being processed faster when preceded by Negative vs. Positive Primes (1,563 ms. vs. 1,547 ms., *t*(294) = 2.91, *p* = .008).